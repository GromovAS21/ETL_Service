# Техническое задание: ELT-сервис для аналитики

## Цель

Реализовать сервис ELT, который будет перекачивать данные из мастер БД (M) в целевую аналитическую БД (A) с частотой раз в 5 минут.

---

## Состав и компоненты

- **Мастер БД (M)**: PostgreSQL, которая выступает источником данных.
    
- **Целевая БД (A)**: PostgreSQL, куда загружаются обработанные данные.
    
- **Redis**: хранилище состояния (хеши от данных, время обновлений).
    
- **ELT-кор**: основной сервис на Python, который обрабатывает файлы из специальной папки `jobs`. Формат файлов абсолютно любой. Один файл в этой папке - один ETL процесс.

---

## Как это работает

1. Аналитик добавляет файл в папку `/jobs`. В нем:
    
    - `CREATE_TABLE_SQL`: сырой SQL-запрос на создание таблицы
        
    - `SELECT_SOURCE_SQL`: запрос к БД M
        
    - `UPSERT_TARGET_SQL`: insert/update запрос к БД A
        
    - `SELECT_KEYS_SQL`: сырой SQL запрос для получения ключей в таблице, чтобы отсеять удаленные.
		
	- `TARGET_TABLE`: Имя созданной таблицы
		
	- `KEY_COLUMNS`: Колонка (или сочетания колонок), по которым нужно сравнить таблицы.
	

2. Каждые 5 минут запускается обновление:
    
    - Поиск новых файлов в папке `/jobs`.
        
    - Выполнение `CREATE_TABLE_SQL` (при необходимости).
        
    - Выполнение `SELECT_SOURCE_SQL`, сравнение с хранящимися хэшами в Redis.
        
    - Если есть изменения — `UPSERT_TARGET_SQL`.
        
3. При удалении записей из БД M, они должны исчезать из A. Для этого проводить сравнение по `KEY COLUMNS`.
    

---
### Логирование:

- Ошибки в `.log`-fайлы.
- Логирование критичных случаев.
